{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80729926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.utils import *\n",
    "from ipynb.fs.full.face_collections import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46695589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces_api(photo_bytes):\n",
    "    client=boto3.client('rekognition')\n",
    "    try:\n",
    "        response = client.detect_faces(\n",
    "            Image={'Bytes': photo_bytes},                                                                       \n",
    "            Attributes = [\"ALL\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't detect faces in %s.\", repr(e))\n",
    "        raise\n",
    "    else:\n",
    "        return {'face_details': response['FaceDetails'], 'face_count': len(response['FaceDetails'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces_crop(photo_name, photo_bytes):\n",
    "    try:\n",
    "        faces = detect_faces_api(photo_bytes)\n",
    "\n",
    "        bounding_boxes = []\n",
    "        for face in faces['face_details']:\n",
    "            bounding_boxes.append(face['BoundingBox'])\n",
    "        cropped_photos = crop_bounding_boxes(photo_name, photo_bytes, bounding_boxes)\n",
    "        \n",
    "        return cropped_photos, bounding_boxes\n",
    "    except Exception as e:\n",
    "        print(repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc164ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class was taken from the AWS demos found in:\n",
    "# - https://docs.aws.amazon.com/rekognition/latest/dg/api-video-roles.html#api-video-roles-all-topics\n",
    "# - https://docs.aws.amazon.com/rekognition/latest/dg/video-analyzing-with-sqs.html\n",
    "# - https://docs.aws.amazon.com/rekognition/latest/dg/faces-sqs-video.html\n",
    "# - https://docs.aws.amazon.com/rekognition/latest/dg/procedure-person-search-videos.html\n",
    "\n",
    "class VideoDetect:\n",
    "    \n",
    "    # Initialize class variables\n",
    "    rek = boto3.client('rekognition')\n",
    "    sqs = boto3.client('sqs')\n",
    "    sns = boto3.client('sns')\n",
    "    roleArn = ''\n",
    "    bucket = ''\n",
    "    video = ''\n",
    "    jobId = ''\n",
    "    startJobId = ''\n",
    "\n",
    "    sqsQueueUrl = ''\n",
    "    snsTopicArn = ''\n",
    "    processType = ''\n",
    "\n",
    "    def __init__(self, role, bucket, video):    \n",
    "        self.roleArn = role\n",
    "        self.bucket = bucket\n",
    "        self.video = video\n",
    "\n",
    "    def GetSQSMessageSuccess(self):\n",
    "        \"\"\"\n",
    "        Function for getting SQS responses.\n",
    "        Constantly monitors the Rekognition SQS queue for a \"Job Success\" message.\n",
    "        jobFound is set to true when the started Rekognition job id is found in the response queue\n",
    "        \"\"\"\n",
    "        jobFound = False\n",
    "        succeeded = False\n",
    "    \n",
    "        dotLine=0\n",
    "        while jobFound == False:\n",
    "            # check sqs responses\n",
    "            sqsResponse = self.sqs.receive_message(QueueUrl=self.sqsQueueUrl, MessageAttributeNames=['ALL'],\n",
    "                                          MaxNumberOfMessages=10)\n",
    "\n",
    "            if sqsResponse:\n",
    "                if 'Messages' not in sqsResponse:\n",
    "                    if dotLine<20:\n",
    "                        print('.', end='')\n",
    "                        dotLine=dotLine+1\n",
    "                    else:\n",
    "                        print()\n",
    "                        dotLine=0    \n",
    "                    sys.stdout.flush()\n",
    "                    # time.sleep(1)\n",
    "                    continue\n",
    "\n",
    "                for message in sqsResponse['Messages']:\n",
    "                    notification = json.loads(message['Body'])\n",
    "                    rekMessage = json.loads(notification['Message'])\n",
    "                    print(rekMessage['JobId'])\n",
    "                    print(rekMessage['Status'])\n",
    "                    \n",
    "                    \n",
    "                    if rekMessage['JobId'] == self.startJobId:\n",
    "                        print('Matching Job Found:' + rekMessage['JobId'])\n",
    "                        jobFound = True\n",
    "                        if (rekMessage['Status']=='SUCCEEDED'):\n",
    "                            succeeded=True\n",
    "\n",
    "                        self.sqs.delete_message(QueueUrl=self.sqsQueueUrl,\n",
    "                                       ReceiptHandle=message['ReceiptHandle'])\n",
    "                    else:\n",
    "                        print(\"Job didn't match:\" +\n",
    "                              str(rekMessage['JobId']) + ' : ' + self.startJobId)\n",
    "                    # Delete the unknown message. Consider sending to dead letter queue\n",
    "                    self.sqs.delete_message(QueueUrl=self.sqsQueueUrl,\n",
    "                                   ReceiptHandle=message['ReceiptHandle'])\n",
    "\n",
    "\n",
    "        return succeeded\n",
    "    \n",
    "    def StartFaceDetection(self):\n",
    "        response = self.rek.start_face_detection(\n",
    "            Video = {\n",
    "                'S3Object': {\n",
    "                    'Bucket': self.bucket, \n",
    "                    'Name': self.video\n",
    "                }\n",
    "            },                                                                                                 \n",
    "            FaceAttributes = \"ALL\",  \n",
    "            NotificationChannel = {\n",
    "                'RoleArn': self.roleArn, \n",
    "                'SNSTopicArn': self.snsTopicArn\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.startJobId=response['JobId']\n",
    "        print('Start Job Id: ' + self.startJobId)\n",
    "        \n",
    "    def StartFaceSearchCollection(self, collection_id):\n",
    "        response = self.rek.start_face_search(Video={'S3Object':{'Bucket':self.bucket,'Name':self.video}},\n",
    "            CollectionId = collection_id,\n",
    "            NotificationChannel = {'RoleArn':self.roleArn, 'SNSTopicArn':self.snsTopicArn})\n",
    "        \n",
    "        self.startJobId=response['JobId']\n",
    "        \n",
    "        print('Start Job Id: ' + self.startJobId)\n",
    "\n",
    "    def GetFaceDetectionResults(self):\n",
    "        maxResults = 1000\n",
    "        paginationToken = ''\n",
    "        finished = False\n",
    "        \n",
    "        faces = {}\n",
    "        while finished == False:\n",
    "            response = self.rek.get_face_detection(JobId=self.startJobId,\n",
    "                                            MaxResults=maxResults,\n",
    "                                            NextToken=paginationToken)\n",
    "            print('-------------------------------------------------------------')\n",
    "            print('Codec: ' + response['VideoMetadata']['Codec'])\n",
    "            print('Duration: ' + str(response['VideoMetadata']['DurationMillis']))\n",
    "            print('Format: ' + response['VideoMetadata']['Format'])\n",
    "            print('Frame rate: ' + str(response['VideoMetadata']['FrameRate']))\n",
    "            print()\n",
    "            \n",
    "            print(response['Faces'])\n",
    "            print('-------------------------------------------------------------')\n",
    "\n",
    "            for faceDetection in response['Faces']:\n",
    "                timestamp = str(faceDetection['Timestamp'])\n",
    "                face = faceDetection['Face']\n",
    "                # face = {\n",
    "                #     'BoundingBox': faceDetection['Face']['BoundingBox'],\n",
    "                #     'Landmarks': faceDetection['Face']['BoundingBox'],\n",
    "                #     'Pose': faceDetection['Face']['Pose'],\n",
    "                #     'Quality': faceDetection['Face']['Quality'],\n",
    "                #     'Confidence': faceDetection['Face']['Confidence']    \n",
    "                # }\n",
    "                if faces.get(timestamp, None) is None:\n",
    "                    faces[timestamp] = [face]\n",
    "                else:\n",
    "                    faces[timestamp].append(face)\n",
    "\n",
    "            if 'NextToken' in response:\n",
    "                paginationToken = response['NextToken']\n",
    "            else:\n",
    "                finished = True\n",
    "                \n",
    "        return faces, response['VideoMetadata']['FrameRate']\n",
    "    \n",
    "    def GetFaceSearchCollectionResults(self):\n",
    "        maxResults = 10\n",
    "        paginationToken = ''\n",
    "\n",
    "        finished = False\n",
    "\n",
    "        faces = {}\n",
    "        while finished == False:\n",
    "            response = self.rek.get_face_search(JobId=self.startJobId,\n",
    "                                        MaxResults=maxResults,\n",
    "                                        NextToken=paginationToken)\n",
    "\n",
    "            print(response['VideoMetadata']['Codec'])\n",
    "            print(str(response['VideoMetadata']['DurationMillis']))\n",
    "            print(response['VideoMetadata']['Format'])\n",
    "            print(response['VideoMetadata']['FrameRate'])\n",
    "            \n",
    "            persons = {}\n",
    "            for personMatch in response['Persons']:\n",
    "                timestamp = str(personMatch['Timestamp'])\n",
    "                p_index = str(personMatch['Person']['Index'])\n",
    "                print('Person Index: ' + p_index)\n",
    "                print('Timestamp: ' + timestamp)\n",
    "                \n",
    "                max_similarity = 0.0\n",
    "                max_face_id = None\n",
    "                if ('FaceMatches' in personMatch):\n",
    "                    for faceMatch in personMatch['FaceMatches']:\n",
    "                        \n",
    "                        if faceMatch['Similarity'] > max_similarity:\n",
    "                            max_similarity = faceMatch['Similarity']\n",
    "                            max_face_id = faceMatch['Face']['FaceId']\n",
    "                    \n",
    "                if max_face_id is not None:\n",
    "                    print('Face ID: ' + max_face_id)\n",
    "                    print('Similarity: ' + str(max_similarity))\n",
    "                    \n",
    "                persons[p_index] = {\n",
    "                    'Details': personMatch['Person']['Face'],\n",
    "                    'FaceId': max_face_id,\n",
    "                    'Similarity': max_similarity\n",
    "                }\n",
    "                \n",
    "                \n",
    "                if faces.get(timestamp, None) is None:\n",
    "                    faces[timestamp] = [persons.copy()]\n",
    "                else:\n",
    "                    faces[timestamp].append(persons)\n",
    "                    \n",
    "            if 'NextToken' in response:\n",
    "                paginationToken = response['NextToken']\n",
    "            else:\n",
    "                finished = True\n",
    "                \n",
    "        return faces, response['VideoMetadata']['FrameRate']\n",
    "        \n",
    "    def CreateTopicandQueue(self):\n",
    "        millis = str(int(round(time.time() * 1)))\n",
    "\n",
    "        #Create SNS topic\n",
    "\n",
    "        snsTopicName = \"AmazonRekognitionExample\" + millis\n",
    "\n",
    "        topicResponse = self.sns.create_topic(Name=snsTopicName)\n",
    "        self.snsTopicArn = topicResponse['TopicArn']\n",
    "\n",
    "        #create SQS queue\n",
    "        sqsQueueName=\"AmazonRekognitionQueue\" + millis\n",
    "        self.sqs.create_queue(QueueName=sqsQueueName)\n",
    "        self.sqsQueueUrl = self.sqs.get_queue_url(QueueName=sqsQueueName)['QueueUrl']\n",
    "\n",
    "        attribs = self.sqs.get_queue_attributes(\n",
    "            QueueUrl = self.sqsQueueUrl,\n",
    "            AttributeNames = ['QueueArn']\n",
    "        )['Attributes']\n",
    "\n",
    "        sqsQueueArn = attribs['QueueArn']\n",
    "\n",
    "        # Subscribe SQS queue to SNS topic\n",
    "        self.sns.subscribe(\n",
    "            TopicArn = self.snsTopicArn,\n",
    "            Protocol = 'sqs',\n",
    "            Endpoint = sqsQueueArn)\n",
    "\n",
    "        #Authorize SNS to write SQS queue \n",
    "        policy = \"\"\"{{\n",
    "            \"Version\":\"2012-10-17\",\n",
    "            \"Statement\":[\n",
    "            {{\n",
    "              \"Sid\":\"MyPolicy\",\n",
    "              \"Effect\":\"Allow\",\n",
    "              \"Principal\" : {{\"AWS\" : \"*\"}},\n",
    "              \"Action\":\"SQS:SendMessage\",\n",
    "              \"Resource\": \"{}\",\n",
    "              \"Condition\":{{\n",
    "                \"ArnEquals\":{{\n",
    "                  \"aws:SourceArn\": \"{}\"\n",
    "                }}\n",
    "              }}\n",
    "            }}\n",
    "            ]\n",
    "        }}\"\"\".format(sqsQueueArn, self.snsTopicArn)\n",
    "\n",
    "        response = self.sqs.set_queue_attributes(\n",
    "            QueueUrl = self.sqsQueueUrl,\n",
    "            Attributes = {\n",
    "                'Policy' : policy\n",
    "                })\n",
    "    def DeleteTopicandQueue(self):\n",
    "        self.sqs.delete_queue(QueueUrl=self.sqsQueueUrl)\n",
    "        self.sns.delete_topic(TopicArn=self.snsTopicArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_video_detection(roleArn, bucket, video):\n",
    "    analyzer=VideoDetect(roleArn, bucket, video)\n",
    "    analyzer.CreateTopicandQueue()\n",
    "    \n",
    "    analyzer.StartFaceDetection()\n",
    "    if analyzer.GetSQSMessageSuccess()==True:\n",
    "        results = analyzer.GetFaceDetectionResults()\n",
    "    \n",
    "    analyzer.DeleteTopicandQueue()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7917aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_video_identification(roleArn, bucket, video, collection_id):\n",
    "    analyzer=VideoDetect(roleArn, bucket, video)\n",
    "    analyzer.CreateTopicandQueue()\n",
    "    \n",
    "    analyzer.StartFaceSearchCollection(collection_id)\n",
    "    \n",
    "    if analyzer.GetSQSMessageSuccess()==True:\n",
    "        results = analyzer.GetFaceSearchCollectionResults()\n",
    "    \n",
    "    analyzer.DeleteTopicandQueue()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_detected_faces(bucket, video, faces, ammend=False, label=False):\n",
    "    obj = get_object(bucket, video, \"../videos/\" + video)\n",
    "    \n",
    "    vid_name = \".\".join(video.split(\".\")[:-1])\n",
    "        \n",
    "    framecount = 0\n",
    "    frame_array = []\n",
    "    \n",
    "    # gather the frames with boxes drawn around the faces\n",
    "    vidcap = cv2.VideoCapture(\"../videos/\" + video)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    width  = vidcap.get(cv2.CAP_PROP_FRAME_WIDTH) \n",
    "    height = vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT) \n",
    "    while(vidcap.isOpened()):\n",
    "        ret, frame = vidcap.read()\n",
    "    \n",
    "        calced_timestamp = vidcap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        if ret == False or calced_timestamp < 0 or (calced_timestamp==0 and framecount!=0):\n",
    "            break\n",
    "        \n",
    "            \n",
    "        h = frame.shape[0]\n",
    "        w = frame.shape[1]\n",
    "        c = frame.shape[2]\n",
    "        \n",
    "        detected = faces.get(str(int(calced_timestamp)), None)\n",
    "        \n",
    "        if detected is not None:\n",
    "            print(\"Detected at: {}, {}\".format(int(calced_timestamp), framecount))\n",
    "            for face in detected:\n",
    "                box = face['BoundingBox']\n",
    "\n",
    "                left = w * box['Left']\n",
    "                top = h * box['Top']\n",
    "                right = (w * box['Width']) + left\n",
    "                bottom = (h * box['Height']) + top\n",
    "\n",
    "                start_point = (int(left), int(top)) \n",
    "                end_point = (int(right), int(bottom)) \n",
    "\n",
    "                # Blue color in BGR \n",
    "                color = (255, 0, 0) \n",
    "\n",
    "                # Line thickness of 2 px \n",
    "                thickness = 3 \n",
    "                \n",
    "                new_frame = cv2.rectangle(frame, start_point, end_point, color, thickness) \n",
    "                if label:\n",
    "                    highest_confidence_emotion = None\n",
    "                    highest_confidence = 0.0\n",
    "                    for emotion in face['Emotions']:\n",
    "                        if emotion['Confidence'] > highest_confidence:\n",
    "                            highest_confidence = emotion['Confidence']\n",
    "                            highest_confidence_emotion = emotion ['Type']\n",
    "\n",
    "                    label= \"\"\"Age Range: {}\\nGender: {}\\nSmile: {}\\nEmotion: {}\n",
    "                       \"\"\".format(\n",
    "                           face['AgeRange'], \n",
    "                           \"{} - {}\".format(face['Gender']['Value'], '%.3f' % face['Gender']['Confidence']),\n",
    "                           \"{} - {}\".format(face['Smile']['Value'], '%.3f' % face['Smile']['Confidence']),\n",
    "                           \"{} - {}\".format(highest_confidence_emotion, '%.3f' % highest_confidence))\n",
    "\n",
    "                    for i, line in enumerate(label.split('\\n')):\n",
    "                        y = int(bottom) + i*20\n",
    "                        cv2.putText(\n",
    "                            new_frame, \n",
    "                            line, \n",
    "                            (int(left), int(y)), \n",
    "                            cv2.FONT_HERSHEY_DUPLEX, \n",
    "                            0.5,\n",
    "                            (0,0,0),\n",
    "                            2\n",
    "                        )\n",
    "                    for i, line in enumerate(label.split('\\n')):\n",
    "                        y = int(bottom) + i*20\n",
    "                        cv2.putText(\n",
    "                            new_frame, \n",
    "                            line, \n",
    "                            (int(left), int(y)), \n",
    "                            cv2.FONT_HERSHEY_DUPLEX, \n",
    "                            0.5,\n",
    "                            (255,255,255),\n",
    "                            1\n",
    "                        )\n",
    "                frame_array.append(new_frame)\n",
    "        else:\n",
    "            if ammend:\n",
    "                # img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                # im_pil = Image.fromarray(img)\n",
    "                \n",
    "                is_success, im_buf_arr = cv2.imencode(\".jpg\", frame)\n",
    "                byte_im = im_buf_arr.tobytes()\n",
    "                # photo_bytes = cv2.imencode('.jpg', frame)[1].tobytes()\n",
    "\n",
    "                ammend_faces = detect_faces_api(byte_im)\n",
    "                \n",
    "                colors = ['blue']*ammend_faces['face_count']\n",
    "                bounding_boxes = []\n",
    "                # gather the bounding boxes so we can easily access them \n",
    "                for face in ammend_faces['face_details']:\n",
    "                    box = face['BoundingBox']\n",
    "\n",
    "                    left = width * box['Left']\n",
    "                    top = height * box['Top']\n",
    "                    right = (width * box['Width']) + left\n",
    "                    bottom = (height * box['Height']) + top\n",
    "\n",
    "                    # set box params\n",
    "                    start_point = (int(left), int(top)) \n",
    "                    end_point = (int(right), int(bottom)) \n",
    "                    color = (255, 0, 0) \n",
    "                    thickness = 2\n",
    "\n",
    "                    # draw box on frames\n",
    "                    new_frame = cv2.rectangle(frame, start_point, end_point, color, thickness) \n",
    "                    \n",
    "                    if label:\n",
    "                        highest_confidence_emotion = None\n",
    "                        highest_confidence = 0.0\n",
    "                        for emotion in face['Emotions']:\n",
    "                            if emotion['Confidence'] > highest_confidence:\n",
    "                                highest_confidence = emotion['Confidence']\n",
    "                                highest_confidence_emotion = emotion ['Type'] \n",
    "                        label= \"\"\"Age Range: {}\\nGender: {}\\nSmile: {}\\nEmotion: {}\n",
    "                           \"\"\".format(\n",
    "                               face['AgeRange'], \n",
    "                               \"{} - {}\".format(face['Gender']['Value'], '%.3f' % face['Gender']['Confidence']),\n",
    "                               \"{} - {}\".format(face['Smile']['Value'], '%.3f' % face['Smile']['Confidence']),\n",
    "                               \"{} - {}\".format(highest_confidence_emotion, '%.3f' % highest_confidence))\n",
    "\n",
    "                        for i, line in enumerate(label.split('\\n')):\n",
    "                            y = int(bottom) + i*20\n",
    "                            cv2.putText(\n",
    "                                new_frame, \n",
    "                                line, \n",
    "                                (int(left), int(y)), \n",
    "                                cv2.FONT_HERSHEY_DUPLEX, \n",
    "                                0.5,\n",
    "                                (0,0,0),\n",
    "                                2\n",
    "                            )\n",
    "                        for i, line in enumerate(label.split('\\n')):\n",
    "                            y = int(bottom) + i*20\n",
    "                            cv2.putText(\n",
    "                                new_frame, \n",
    "                                line, \n",
    "                                (int(left), int(y)), \n",
    "                                cv2.FONT_HERSHEY_DUPLEX, \n",
    "                                0.5,\n",
    "                                (255,255,255),\n",
    "                                1\n",
    "                            )\n",
    "\n",
    "                print(\"Ammended frame: {}\".format(framecount))\n",
    "                frame_array.append(new_frame)\n",
    "            else:\n",
    "                frame_array.append(frame)\n",
    "\n",
    "        framecount+=1\n",
    "    print(framecount)\n",
    "    print(len(frame_array))\n",
    "    # compile the frames into a video\n",
    "    new_vid = \"../videos/detected_labeled_\" if label else \"../videos/detected_\"\n",
    "    \n",
    "    out = cv2.VideoWriter(\n",
    "        new_vid + video, \n",
    "        cv2.VideoWriter_fourcc(*'DIVX'), \n",
    "        fps, \n",
    "        (w, h)\n",
    "    )\n",
    "    for i in range(len(frame_array)):\n",
    "        out.write(frame_array[i])\n",
    "    out.release()\n",
    "\n",
    "    vidcap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_crowd_size(bucket, video, faces, ammend=False, label_preamble='', detect_distinct=False):\n",
    "    obj = get_object(bucket, video, \"../videos/\" + video)\n",
    "    \n",
    "    vid_name = \".\".join(video.split(\".\")[:-1])\n",
    "        \n",
    "    framecount = 0\n",
    "    frame_array = []\n",
    "    \n",
    "    # gather the frames with boxes drawn around the faces\n",
    "    vidcap = cv2.VideoCapture(\"../videos/\" + video)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    width  = vidcap.get(cv2.CAP_PROP_FRAME_WIDTH) \n",
    "    height = vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT) \n",
    "    \n",
    "    # set up a collection for detect distinct\n",
    "    if detect_distinct:\n",
    "        delete_collection(vid_name)\n",
    "        create_collection(vid_name)\n",
    "        dest_folder=\"../images/cropped/\"\n",
    "        distinct_faces = 0\n",
    "        identified_faces = {}\n",
    "        bounding_boxes = []\n",
    "        times = []\n",
    "    \n",
    "    prev_timestamp = 0\n",
    "    while(vidcap.isOpened()):\n",
    "        ret, frame = vidcap.read()\n",
    "        calced_timestamp = vidcap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        \n",
    "        if ret == False or calced_timestamp < 0 or (calced_timestamp==0 and framecount!=0):\n",
    "            break\n",
    "        \n",
    "        h = frame.shape[0]\n",
    "        w = frame.shape[1]\n",
    "        c = frame.shape[2]\n",
    "        \n",
    "        detected = faces.get(str(int(calced_timestamp)), None)\n",
    "                \n",
    "        new_frame = frame.copy()\n",
    "        if detected is not None:\n",
    "            if not detect_distinct:\n",
    "                label = '{}: {}'.format(label_preamble, len(detected))\n",
    "                cv2.putText(\n",
    "                    new_frame, \n",
    "                    label, \n",
    "                    (int(w/2), int(h*3/4)), \n",
    "                    cv2.FONT_HERSHEY_DUPLEX, \n",
    "                    1,\n",
    "                    (0,0,0),\n",
    "                    2)\n",
    "                cv2.putText(\n",
    "                    new_frame, \n",
    "                    label, \n",
    "                    (int(w/2), int(h*3/4)), \n",
    "                    cv2.FONT_HERSHEY_DUPLEX, \n",
    "                    1,\n",
    "                    (255,255,255),\n",
    "                    1)\n",
    "                print('{}: {}'.format(framecount, label))\n",
    "            else:\n",
    "                is_success, im_buf_arr = cv2.imencode('.jpg', frame)\n",
    "                byte_im = im_buf_arr.tobytes()\n",
    "                \n",
    "                photo_name = vid_name + '_cropped.jpg'\n",
    "                cropped_faces = detect_faces_crop(photo_name, byte_im)\n",
    "                \n",
    "                bounding_boxes = []\n",
    "                labels = []\n",
    "                \n",
    "                idx = 0\n",
    "                for cropped_face in cropped_faces[0]:\n",
    "                    existing = search_faces_by_image(\n",
    "                        vid_name,\n",
    "                        dest_folder + cropped_face,\n",
    "                        70,\n",
    "                        10\n",
    "                    )\n",
    "                    face_id = None\n",
    "                    recorded = False\n",
    "                    for e in existing:\n",
    "                        if e['FaceId'] in identified_faces.keys():\n",
    "                            recorded = True\n",
    "                            break\n",
    "\n",
    "                    if len(existing) == 0:\n",
    "                        distinct_faces += 1\n",
    "                        res = add_to_collection(\n",
    "                            vid_name,\n",
    "                            dest_folder + cropped_face,\n",
    "                            1,\n",
    "                            cropped_face\n",
    "                        )\n",
    "                        face_id = res[0][0]['FaceId']\n",
    "                    elif recorded:\n",
    "                        for e in existing:\n",
    "                            if e['FaceId'] in identified_faces.keys():\n",
    "                                face_id = e['FaceId']\n",
    "                    else:\n",
    "                        face_id = existing[0]['FaceId']\n",
    "                        \n",
    "                    if face_id is not None:\n",
    "                        if face_id not in list(identified_faces.keys()):\n",
    "                            identified_faces[face_id] = 0.0\n",
    "                        else:\n",
    "                            identified_faces[face_id] += (calced_timestamp-prev_timestamp)/1000\n",
    "                    label = 'Time in video: {}'.format('%.3f' % identified_faces[face_id])\n",
    "                    box = cropped_faces[1][idx]\n",
    "                    \n",
    "                    bounding_boxes.append(box)\n",
    "                    labels.append(label)\n",
    "\n",
    "                    left = w * box['Left']\n",
    "                    top = h * box['Top']\n",
    "                    right = (w * box['Width']) + left\n",
    "                    bottom = (h * box['Height']) + top\n",
    "\n",
    "                    start_point = (int(left), int(top)) \n",
    "                    end_point = (int(right), int(bottom)) \n",
    "\n",
    "                    # Blue color in BGR \n",
    "                    color = (255, 0, 0) \n",
    "\n",
    "                    # Line thickness of 2 px \n",
    "                    thickness = 3 \n",
    "\n",
    "                    new_frame = cv2.rectangle(new_frame, start_point, end_point, color, thickness) \n",
    "                    cv2.putText(\n",
    "                        new_frame, \n",
    "                        label, \n",
    "                        (int(left), int(bottom)), \n",
    "                        cv2.FONT_HERSHEY_DUPLEX, \n",
    "                        1,\n",
    "                        (255,0,0),\n",
    "                        2)\n",
    "                    cv2.putText(\n",
    "                        new_frame, \n",
    "                        label, \n",
    "                        (int(left), int(bottom)), \n",
    "                        cv2.FONT_HERSHEY_DUPLEX, \n",
    "                        1,\n",
    "                        (255,255,255),\n",
    "                        1)\n",
    "                    idx += 1\n",
    "            frame_array.append(new_frame)\n",
    "        else:\n",
    "            if ammend:\n",
    "                is_success, im_buf_arr = cv2.imencode('.jpg', frame)\n",
    "                byte_im = im_buf_arr.tobytes()\n",
    "                ammend_faces = detect_faces_api(byte_im)\n",
    "                \n",
    "                if not detect_distinct:\n",
    "                    label = '{}: {}'.format(label_preamble, len(ammend_faces['face_details']))\n",
    "                    cv2.putText(\n",
    "                        new_frame, \n",
    "                        label, \n",
    "                        (int(w/2), int(h*3/4)), \n",
    "                        cv2.FONT_HERSHEY_DUPLEX, \n",
    "                        1,\n",
    "                        (0,0,0),\n",
    "                        2)\n",
    "                    cv2.putText(\n",
    "                        new_frame, \n",
    "                        label, \n",
    "                        (int(w/2), int(h*3/4)), \n",
    "                        cv2.FONT_HERSHEY_DUPLEX, \n",
    "                        1,\n",
    "                        (255,255,255),\n",
    "                        1)\n",
    "                    print('Ammended {} label: {}'.format(framecount, label))\n",
    "                else:\n",
    "                    photo_name = vid_name + '_cropped.jpg'\n",
    "                    cropped_faces = detect_faces_crop(photo_name, byte_im)\n",
    "                    \n",
    "                    idx = 0\n",
    "                    for cropped_face in cropped_faces[0]:\n",
    "                        existing = search_faces_by_image(\n",
    "                            vid_name,\n",
    "                            dest_folder + cropped_face,\n",
    "                            70,\n",
    "                            10\n",
    "                        )\n",
    "                        face_id = None\n",
    "                        recorded = False\n",
    "                        for e in existing:\n",
    "                            if e['FaceId'] in identified_faces.keys():\n",
    "                                recorded = True\n",
    "                                break\n",
    "                        \n",
    "                        if len(existing) == 0:\n",
    "                            distinct_faces += 1\n",
    "                            res = add_to_collection(\n",
    "                                vid_name,\n",
    "                                dest_folder + cropped_face,\n",
    "                                1,\n",
    "                                cropped_face\n",
    "                            )\n",
    "                            face_id = res[0][0]['FaceId']\n",
    "                        elif recorded:\n",
    "                            for e in existing:\n",
    "                                if e['FaceId'] in identified_faces.keys():\n",
    "                                    face_id = e['FaceId']\n",
    "                        else:\n",
    "                            face_id = existing[0]['FaceId']\n",
    "\n",
    "                        if face_id is not None:\n",
    "                            if face_id not in list(identified_faces.keys()):\n",
    "                                identified_faces[face_id] = 0.0\n",
    "                            else:\n",
    "                                identified_faces[face_id] += (calced_timestamp-prev_timestamp)/1000\n",
    "                        label = 'Time in video: {}'.format('%.3f' % identified_faces[face_id])\n",
    "                        box = cropped_faces[1][idx]\n",
    "\n",
    "                        left = w * box['Left']\n",
    "                        top = h * box['Top']\n",
    "                        right = (w * box['Width']) + left\n",
    "                        bottom = (h * box['Height']) + top\n",
    "\n",
    "                        start_point = (int(left), int(top)) \n",
    "                        end_point = (int(right), int(bottom)) \n",
    "\n",
    "                        # Blue color in BGR \n",
    "                        color = (255, 0, 0) \n",
    "\n",
    "                        # Line thickness of 2 px \n",
    "                        thickness = 3 \n",
    "\n",
    "                        new_frame = cv2.rectangle(new_frame, start_point, end_point, color, thickness) \n",
    "                        cv2.putText(\n",
    "                            new_frame, \n",
    "                            label, \n",
    "                            (int(left), int(bottom)), \n",
    "                            cv2.FONT_HERSHEY_DUPLEX, \n",
    "                            1,\n",
    "                            (255,0,0),\n",
    "                            2)\n",
    "                        cv2.putText(\n",
    "                            new_frame, \n",
    "                            label, \n",
    "                            (int(left), int(bottom)), \n",
    "                            cv2.FONT_HERSHEY_DUPLEX, \n",
    "                            1,\n",
    "                            (255,255,255),\n",
    "                            1)\n",
    "                        idx += 1\n",
    "                frame_array.append(new_frame)\n",
    "            else:\n",
    "                if detect_distinct:\n",
    "                    box_idx = 0\n",
    "                    for box in bounding_boxes:\n",
    "                        \n",
    "                        label = labels[box_idx]\n",
    "                        left = w * box['Left']\n",
    "                        top = h * box['Top']\n",
    "                        right = (w * box['Width']) + left\n",
    "                        bottom = (h * box['Height']) + top\n",
    "\n",
    "                        start_point = (int(left), int(top)) \n",
    "                        end_point = (int(right), int(bottom)) \n",
    "\n",
    "                        # Blue color in BGR \n",
    "                        color = (255, 0, 0) \n",
    "\n",
    "                        # Line thickness of 2 px \n",
    "                        thickness = 3 \n",
    "\n",
    "                        new_frame = cv2.rectangle(new_frame, start_point, end_point, color, thickness) \n",
    "                        cv2.putText(\n",
    "                            new_frame, \n",
    "                            label, \n",
    "                            (int(left), int(bottom)), \n",
    "                            cv2.FONT_HERSHEY_DUPLEX, \n",
    "                            1,\n",
    "                            (255,0,0),\n",
    "                            2)\n",
    "                        cv2.putText(\n",
    "                            new_frame, \n",
    "                            label, \n",
    "                            (int(left), int(bottom)), \n",
    "                            cv2.FONT_HERSHEY_DUPLEX, \n",
    "                            1,\n",
    "                            (255,255,255),\n",
    "                            1)\n",
    "                        box_idx += 1\n",
    "                    frame_array.append(new_frame)\n",
    "                else:\n",
    "                    frame_array.append(frame)\n",
    "\n",
    "        framecount+=1\n",
    "        prev_timestamp = calced_timestamp\n",
    "        if detect_distinct:\n",
    "            print('identified_faces at {}: {}'.format(str(int(calced_timestamp)), identified_faces))\n",
    "    \n",
    "    if detect_distinct:\n",
    "        print('------------------')\n",
    "        print('Detected {} distinct faces'.format(distinct_faces))\n",
    "        print('identified_faces: {}'.format(identified_faces))\n",
    "        print('------------------')\n",
    "        delete_collection(vid_name)\n",
    "                        \n",
    "    # compile the frames into a video\n",
    "    new_vid = '../videos/detected_crowd_count_'\n",
    "    \n",
    "    if len(frame_array) > 0:\n",
    "        out = cv2.VideoWriter(\n",
    "            new_vid + video, \n",
    "            cv2.VideoWriter_fourcc(*'DIVX'), \n",
    "            fps, \n",
    "            (w, h)\n",
    "        )\n",
    "        for i in range(len(frame_array)):\n",
    "            out.write(frame_array[i])\n",
    "            \n",
    "    out.release()\n",
    "        \n",
    "    vidcap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca31367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_identification_results(video, bucket, faces, collection_id):\n",
    "    obj = get_object(bucket, video, \"../videos/\" + video)\n",
    "    \n",
    "    vid_name = \".\".join(video.split(\".\")[:-1])\n",
    "        \n",
    "    framecount = 0\n",
    "    frame_array = []\n",
    "    \n",
    "    # gather the frames with boxes drawn around the faces\n",
    "    vidcap = cv2.VideoCapture(\"../videos/\" + video)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    width  = vidcap.get(cv2.CAP_PROP_FRAME_WIDTH) \n",
    "    height = vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT) \n",
    "    \n",
    "    authorized = {}\n",
    "    unauthorized = 0\n",
    "    while(vidcap.isOpened()):\n",
    "        ret, frame = vidcap.read()\n",
    "        \n",
    "        calced_timestamp = vidcap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        \n",
    "        if ret == False or calced_timestamp < 0 or (calced_timestamp==0 and framecount!=0):\n",
    "            break\n",
    "        \n",
    "        h = frame.shape[0]\n",
    "        w = frame.shape[1]\n",
    "        c = frame.shape[2]\n",
    "        \n",
    "        detected = faces.get(str(int(calced_timestamp)), None)\n",
    "        \n",
    "        if detected is not None:    \n",
    "            print(\"Detected at: {}\".format(calced_timestamp))\n",
    "            for people in detected:\n",
    "                for person in people:\n",
    "                    face_id = people[person]['FaceId']\n",
    "                    \n",
    "                    if face_id is None:\n",
    "                        unauthorized += 1\n",
    "                    else:\n",
    "                        # make sure we get only the distinct name\n",
    "                        face_name = search_faces(collection_id, face_id)[0]['ExternalImageId'].split('_')[0]\n",
    "                        authorized[face_name] = people[person]['Details']['BoundingBox']\n",
    "        \n",
    "        if detected is not None:\n",
    "            unauthorized = len(detected) - len(authorized)\n",
    "        \n",
    "        new_frame = frame.copy()\n",
    "        for face_id in authorized:\n",
    "            box = authorized[face_id]\n",
    "\n",
    "            left = w * box['Left']\n",
    "            top = h * box['Top']\n",
    "            right = (w * box['Width']) + left\n",
    "            bottom = (h * box['Height']) + top\n",
    "\n",
    "            start_point = (int(left), int(top)) \n",
    "            end_point = (int(right), int(bottom)) \n",
    "\n",
    "            # Blue color in BGR \n",
    "            color = (255, 0, 0) \n",
    "\n",
    "            # Line thickness of 2 px \n",
    "            thickness = 3 \n",
    "\n",
    "            new_frame = cv2.rectangle(new_frame, start_point, end_point, color, thickness) \n",
    "        \n",
    "        label = 'Authorized: {}'.format(' '.join(list(authorized.keys())))\n",
    "        cv2.putText(\n",
    "            new_frame, \n",
    "            label, \n",
    "            (int(w/4), int(h*3/4)), \n",
    "            cv2.FONT_HERSHEY_DUPLEX, \n",
    "            0.5,\n",
    "            (255,0,0),\n",
    "            2)\n",
    "        cv2.putText(\n",
    "            new_frame, \n",
    "            label, \n",
    "            (int(w/4), int(h*3/4)), \n",
    "            cv2.FONT_HERSHEY_DUPLEX, \n",
    "            0.5,\n",
    "            (255,255,255),\n",
    "            1)\n",
    "        frame_array.append(new_frame)\n",
    "        \n",
    "        label = 'Unauthorized: {}'.format(unauthorized)\n",
    "        cv2.putText(\n",
    "            new_frame, \n",
    "            label, \n",
    "            (int(w/4), int(h*3/4) + 20), \n",
    "            cv2.FONT_HERSHEY_DUPLEX, \n",
    "            0.5,\n",
    "            (0,0,255),\n",
    "            2)\n",
    "        cv2.putText(\n",
    "            new_frame, \n",
    "            label, \n",
    "            (int(w/4), int(h*3/4) + 20), \n",
    "            cv2.FONT_HERSHEY_DUPLEX, \n",
    "            0.5,\n",
    "            (255,255,255),\n",
    "            1)\n",
    "        frame_array.append(new_frame)\n",
    "            \n",
    "        framecount += 1\n",
    "        \n",
    "    print(authorized)\n",
    "                        \n",
    "    # compile the frames into a video\n",
    "    new_vid = '../videos/detect_'\n",
    "    \n",
    "    out = cv2.VideoWriter(\n",
    "        new_vid + video, \n",
    "        cv2.VideoWriter_fourcc(*'DIVX'), \n",
    "        fps, \n",
    "        (w, h)\n",
    "    )\n",
    "    for i in range(len(frame_array)):\n",
    "        out.write(frame_array[i])\n",
    "    out.release()\n",
    "\n",
    "    vidcap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_capacity(bucket, video, faces, ammend=False, capacity=5):\n",
    "    obj = get_object(bucket, video, \"../videos/\" + video)\n",
    "    \n",
    "    vid_name = \".\".join(video.split(\".\")[:-1])\n",
    "        \n",
    "    framecount = 0\n",
    "    frame_array = []\n",
    "    \n",
    "    # gather the frames with boxes drawn around the faces\n",
    "    vidcap = cv2.VideoCapture(\"../videos/\" + video)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    width  = vidcap.get(cv2.CAP_PROP_FRAME_WIDTH) \n",
    "    height = vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT) \n",
    "    while(vidcap.isOpened()):\n",
    "        ret, frame = vidcap.read()\n",
    "    \n",
    "        calced_timestamp = vidcap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        if ret == False or calced_timestamp < 0 or (calced_timestamp==0 and framecount!=0):\n",
    "            break\n",
    "        \n",
    "            \n",
    "        h = frame.shape[0]\n",
    "        w = frame.shape[1]\n",
    "        c = frame.shape[2]\n",
    "        \n",
    "        detected = faces.get(str(int(calced_timestamp)), None)\n",
    "        \n",
    "        new_frame = frame.copy()\n",
    "        if detected is not None:\n",
    "            print(\"Detected at: {}, {}\".format(int(calced_timestamp), framecount))\n",
    "            for face in detected:\n",
    "                box = face['BoundingBox']\n",
    "\n",
    "                left = w * box['Left']\n",
    "                top = h * box['Top']\n",
    "                right = (w * box['Width']) + left\n",
    "                bottom = (h * box['Height']) + top\n",
    "\n",
    "                start_point = (int(left), int(top)) \n",
    "                end_point = (int(right), int(bottom)) \n",
    "\n",
    "                # Blue color in BGR \n",
    "                color = (255, 0, 0) \n",
    "\n",
    "                # Line thickness of 2 px \n",
    "                thickness = 3 \n",
    "                \n",
    "                new_frame = cv2.rectangle(new_frame, start_point, end_point, color, thickness) \n",
    "\n",
    "            label = 'Capacity: {}'.format(capacity - len(detected))\n",
    "            cv2.putText(\n",
    "                new_frame, \n",
    "                label, \n",
    "                (40, 40), \n",
    "                cv2.FONT_HERSHEY_DUPLEX, \n",
    "                2,\n",
    "                (0,0,0),\n",
    "                3\n",
    "            )\n",
    "            frame_array.append(new_frame)\n",
    "        else:\n",
    "            if ammend:\n",
    "                # img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                # im_pil = Image.fromarray(img)\n",
    "                \n",
    "                is_success, im_buf_arr = cv2.imencode(\".jpg\", new_frame)\n",
    "                byte_im = im_buf_arr.tobytes()\n",
    "                # photo_bytes = cv2.imencode('.jpg', frame)[1].tobytes()\n",
    "\n",
    "                ammend_faces = detect_faces_api(byte_im)\n",
    "                \n",
    "                colors = ['blue']*ammend_faces['face_count']\n",
    "                bounding_boxes = []\n",
    "                # gather the bounding boxes so we can easily access them \n",
    "                for face in ammend_faces['face_details']:\n",
    "                    box = face['BoundingBox']\n",
    "\n",
    "                    left = width * box['Left']\n",
    "                    top = height * box['Top']\n",
    "                    right = (width * box['Width']) + left\n",
    "                    bottom = (height * box['Height']) + top\n",
    "\n",
    "                    # set box params\n",
    "                    start_point = (int(left), int(top)) \n",
    "                    end_point = (int(right), int(bottom)) \n",
    "                    color = (255, 0, 0) \n",
    "                    thickness = 2\n",
    "\n",
    "                    # draw box on frames\n",
    "                    new_frame = cv2.rectangle(new_frame, start_point, end_point, color, thickness) \n",
    "                    \n",
    "                label = 'Capacity: {}'.format(capacity - len(ammend_faces))\n",
    "                cv2.putText(\n",
    "                    new_frame, \n",
    "                    label,\n",
    "                    (40, 40), \n",
    "                    cv2.FONT_HERSHEY_DUPLEX, \n",
    "                    2,\n",
    "                    (0,0,0),\n",
    "                    3\n",
    "                )\n",
    "                frame_array.append(new_frame)\n",
    "                print(\"Ammended frame: {}\".format(framecount))\n",
    "            else:\n",
    "                frame_array.append(frame)\n",
    "\n",
    "        framecount+=1\n",
    "    # compile the frames into a video\n",
    "    new_vid = \"../videos/detected_labeled_\" if label else \"../videos/detected_\"\n",
    "    \n",
    "    out = cv2.VideoWriter(\n",
    "        new_vid + video, \n",
    "        cv2.VideoWriter_fourcc(*'DIVX'), \n",
    "        fps, \n",
    "        (w, h)\n",
    "    )\n",
    "    for i in range(len(frame_array)):\n",
    "        out.write(frame_array[i])\n",
    "    out.release()\n",
    "\n",
    "    vidcap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
